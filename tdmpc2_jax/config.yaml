hydra:
  job:
    name: ${env.env_id}_s${seed}
  run:
    dir: outputs/${hydra.job.name}/${now:%Y-%m-%d_%H-%M-%S}

seed: 0
max_steps: 1_000_000
save_interval_steps: 100000
log_interval_steps: 10001

env:
  backend: raisim
  env_id: test
  num_envs: 4
  utd_ratio: 0.5
  asynchronous: True

  raisim: 
    obs_type: multimodal
    resource_path: /home/gmmyung/Develop/raisimLib/rsc
    urdf_path: /anymal/urdf/anymal_depth_sensor.urdf
    visualize_seed: 0
    render: True
    num_envs: 1
    eval_every_n: 1 
    num_threads: 1
    simulation_dt: 0.0025
    control_dt: 0.05
    max_time: 6.0
    action_std: 0.3
    reward:
      forwardVel:
        coeff: 0.3
      torque:
        coeff: -4e-5
    p_gain: 80.0
    d_gain: 2
    server_port: 8081

encoder:
  encoder_dim: 256
  fuse_dim: 256
  num_encoder_layers: 2
  learning_rate: 1e-4
  tabulate: True

world_model:
  mlp_dim: 512
  latent_dim: 512
  value_dropout: 0.01
  num_value_nets: 5
  num_bins: 101
  symlog_min: -10
  symlog_max: 10
  symlog_obs: False
  simnorm_dim: 8
  learning_rate: 3e-4
  predict_continues: False
  dtype: float16
  max_grad_norm: 20
  tabulate: True

tdmpc2:
  # Planning
  mpc: True
  horizon: 3
  mppi_iterations: 6
  population_size: 512
  policy_prior_samples: 24
  num_elites: 64
  min_plan_std: 0.05
  max_plan_std: 2
  temperature: 0.5
  # Optimization
  batch_size: 256
  discount: 0.99
  rho: 0.5
  consistency_coef: 20
  reward_coef: 0.1
  continue_coef: 1.0
  value_coef: 0.1
  entropy_coef: 1e-4
  tau: 0.01
